# Visual Affective Computing Survey

--------------------------------------------------------------
## ğŸ“– Reviews

* [2022 TPAMI] **Affective Image Content Analysis: Two Decades Review and New Perspectives**, Sicheng Zhao; Xingxu Yao; Jufeng Yang; Guoli Jia; Guiguang Ding; Tat-Seng Chua; BjÃ¶rn W. Schuller; Kurt Keutzer
  [[Paper](https://ieeexplore.ieee.org/document/9472932)]


## ğŸ“Š Datasets

* [2023 CVPR] ğŸ”– [] ğŸ”– **iMiGUE: An ldentity-Free Video Dataset for Micro-Gesture Understanding and Emotion Analysis**,
  [[Paper]()] [[Code]()]

* [2023 Information Fusion] ğŸ”– [Singapore University of Technology and Design] ğŸ”– **EmoMV: Affective music-video correspondence learning datasets for classification and retrieval**, Ha Thi Phuong Thao, Gemma Roig, Dorien Herremans 
  [[Paper](https://doi.org/10.1016/j.inffus.2022.10.002)] [[Code](https://github.com/ivyha010/EmoMV)]

* [2023 ACM MM] ğŸ”– [Institute of Automation, China] ğŸ”– **Sensing Micro-Motion Human Patterns using Multimodal mmRadar and Video Signal for Affective and Psychological Intlligence**, Yiwei Ru, Peipei Li, Muyi Sun, Yunlong Wang, Kunbo Zhang, Qi Li, Zhaofeng He, Zhenan Sun
  [[Paper](https://dl.acm.org/doi/10.1145/3581783.3611754)]  [[Code]()]

* [2021 ACM MM] ğŸ”– [Alibaba Group, China] ğŸ”– **Pairwise Emotional Relationship Recognition in Drama Videos: Dataset and Benchmark**, Xun Gao, Yin Zhao, Jie Zhang, jun Cai
  [[Paper](https://dl.acm.org/doi/10.1145/3474085.3475493)]  [[Code]()]

* [2020 ACM MM] ğŸ”– ğŸ”– **MEmoR:A Dataset for Multimodal Emotion Reasoning in Videos**,  
  [[Paper]()]  [[Code]()]
  
--------------------------------------------------------------
## ğŸ“· Image Tasks
### ğŸŒ Open-world Image Emotion Classification
```
  Classification, Image Similarity Metric, Emotion Distribution, 
```

* [2023 ACM MM] **Progressive Visual Content Understanding Network for Image Emotion Classification**, Jicai Pan, Shangfei Wang
  [[Paper](https://dl.acm.org/doi/abs/10.1145/3581783.3612186)]


* [2023 ACM MM] **StyleEDL: Style-Guided High-order Attention Network for Image Emotion Distribution Learning**, Peiguang Jing, Xianyi Liu, Ji Wang, Yinwei Wei, Liqiang Nie, Yuting Su
  [[Paper](https://dl.acm.org/doi/abs/10.1145/3581783.3612040)]

* [2023 TMM] **Multiscale Emotion Representation Learning for Affective Image Recognition**, Haimin Zhang; Min Xu
  [[Paper](https://ieeexplore.ieee.org/document/9693105)]


* [2022 IJCAI] **Automatic Recognition of Emotional Subgroups in Images**, Emmeke Veltmeijer , Charlotte Gerritsen and Koen Hindriks
  [[Paper](https://doi.org/10.24963/ijcai.2022/190)] [[Code](https://github.com/Emmekea/emotional-subgroup-recognition)] 

* [2021 TMM] **Weakly Supervised Emotion Intensity Prediction for Recognition of Emotions in Images**, Haimin Zhang; Min Xu
  [[Paper](https://ieeexplore.ieee.org/document/9136892)]

* [2021 TMM] **Image-Text Multimodal Emotion Classification via Multi-View Attentional Network**, Xiaocui Yang; Shi Feng; Daling Wang; Yifei Zhang
  [[Paper](https://ieeexplore.ieee.org/document/9246699)]
  
* [2019 AAAI] **CycleEmotionGAN: Emotional Semantic Consistency Preserved CycleGAN for Adapting Image Emotions**, Sicheng Zhao, Chuang Lin, Pengfei Xu, Sendong Zhao, Yuchen Guo, Ravi Krishna, Guiguang Ding, Kurt Keutzer
  [[Paper](https://doi.org/10.1609/aaai.v33i01.33012620)]


* [2019 AAAI] **Structured and Sparse Annotations for Image Emotion Distribution Learning**, Haitao Xiong, Hongfu Liu, Bineng Zhong, Yun Fu
  [[Paper](https://doi.org/10.1609/aaai.v33i01.3301363)]



### ğŸŒ Open-world Emotion Image Captioning

* [2022 CVPR] **It is Okay to Not Be Okay: Overcoming Emotional Bias in Affective Image Captioning by Contrastive Data Collection**, Mohamed, Youssef; Khan, Faizan Farooq; Haydarov, Kilichbek; Elhoseiny, Mohamed
  [[Paper](https://ieeexplore.ieee.org/document/9878621)] [[Code](https://www.artemisdataset-v2.org/)]

* [2021 ACM MM] **Similar Scenes Arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning**, Guodun Li, Yuchen Zhai, Zehao Lin, Yin Zhang
  [[Paper](https://dl.acm.org/doi/10.1145/3474085.3475662)] 


* [2020 ACM MM] **AffectI: A Game for Diverse, Reliable, and Efficient Affective Image Annotation**, Xingkun Zuo, Jiyi Li, Qili Zhou, Jianjun Li, Xiaoyang Mao
  [[Paper](https://dl.acm.org/doi/10.1145/3394171.3413744)] 


* [2019 ACM MM] **Towards Increased Accessibility of Meme Images with the Help of Rich Face Emotion Captions**, K R Prajwal, C V Jawahar, Ponnurangam Kumaraguru
  [[Paper](https://dl.acm.org/doi/10.1145/3343031.3350939)] [[Code](http://precog.iiitd.edu.in/research/social-media-4-all)]

  
### ğŸŒ Open-world Emotion Image Retrieval

* [2021 TMM] **APSE: Attention-Aware Polarity-Sensitive Embedding for Emotion-Based Image Retrieval**, Xingxu Yao; Sicheng Zhao; Yu-Kun Lai; Dongyu She; Jie Liang; Jufeng Yang
  [[Paper](https://ieeexplore.ieee.org/document/9281019)]

* [2021 TMM] **Adaptive Deep Metric Learning for Affective Image Retrieval and Classification**, Xingxu Yao; Dongyu She; Haiwei Zhang; Jufeng Yang; Ming-Ming Cheng; Liang Wang
  [[Paper](https://ieeexplore.ieee.org/document/9113756)]

* [2020 ACM MM] **Attention-Aware Polarity Sensitive Embedding for Affective Image Retrieval**, Sicheng Zhao, Yaxian Li, Xingxu Yao, Weizhi Nie, Pengfei Xu, Jufeng Yang, Kurt Keutzer
  [[Paper](https://dl.acm.org/doi/10.1145/3394171.3413776)]

  
* [2019 ICCV] **Emotion-Based End-to-End Matching Between Image and Music in Valence-Arousal Space**, Xingxu Yao; Dongyu She; Sicheng Zhao; Jie Liang; Yu-Kun Lai; Jufeng Yang
  [[Paper](https://ieeexplore.ieee.org/document/9008797)]


### ğŸŒ Open-world Emotion Image Editing

* [2021 TMM] **Emotion Attention-Aware Collaborative Deep Reinforcement Learning for Image Cropping**, Xiaoyan Zhang; Zhuopeng Li; Jianmin Jiang
  [[Paper](https://ieeexplore.ieee.org/document/9158370)]


### ğŸ˜Š Macro Facial Expression Recognition


  
### ğŸ˜Š Micro Facial Expression Recognition 


### ğŸ˜Š Emotion-Image-based Talking Face Generation

* [2022 TMM] **Speech Driven Talking Face Generation From a Single Image and an Emotion Condition**, Sefik Emre Eskimez; You Zhang; Zhiyao Duan
  [[Paper](https://ieeexplore.ieee.org/document/9496264)]




------------------------------------------------------------------
## ğŸ¬ Video Tasks







### ğŸŒ Open-world Emotional Video Captioning

* [2023 ACM MM] ğŸ”– ğŸ”– **Emotion-Prior Awareness Network for Emotional Video Captioning**, Peipei Song, Dan Guo, Xun Yang, Shengeng Tang, Erkun Yang, Meng Wang
  [[Paper](https://dl.acm.org/doi/abs/10.1145/3581783.3611726)]  [[Code](https://github.com/songpipi/EPAN)]

* [2023 ACM MM] ğŸ”– [University of Canberra, Australia] ğŸ”– **Efficient Labelling of Affective Video Datasets via Few-Shot & Multi-Task Contrastive Learning**, Ravikiran Parameshwara, brahim Radwan, Akshay Asthana, Iman Abbasnejad, Ramanathan Subramanian, Roland Goecke 
  [[Paper](https://dl.acm.org/doi/10.1145/3581783.3613784)]  [[Code](https://github.com/ravikiranrao/MTCLAR-FSL)]
  

### ğŸŒ Open-world Video Emotion Recognition 

* [2022 ACM MM] ğŸ”– [University of Chinese Academy of Sciences, China] ğŸ”– **Feeling Without Sharing: A Federated Video Emotion Recognition Framework Via Privacy-Agnostic Hybrid Aggregation**, Fan Qi, Zixin Zhang, Xianshan Yang, Huaiwen Zhang, Changsheng Xu
  [[Paper](https://dl.acm.org/doi/10.1145/3503161.3548278)]
  
* [2022 ACM MM] ğŸ”– [University of Science and Technology of China] ğŸ”– **Representation Learning through Multimodal Attention and Time-SyncComments for Affective Video Content Analysis**,  Jicai Pan, Shangfei Wang, Lin Fang
  [[Paper](https://dl.acm.org/doi/10.1145/3503161.3548018)] 

* [2021 ACM MM] ğŸ”– [Peng Cheng Laboratory, China] ğŸ”– **Zero-shot Video Emotion Recognition via Multimodal Protagonist-aware Transformer Network**, Fan Qi, Xiaoshan Yang, Changsheng Xu
  [[Paper](https://dl.acm.org/doi/10.1145/3474085.3475647)]
  
* [2020 AAAI] ğŸ”– [Nankai University, China] ğŸ”– **An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos**, Sicheng Zhao, Yunsheng Ma, Yang Gu, Jufeng Yang, Tengfei Xing, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer
  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/5364)] [[Code](https://github.com/maysonma/VAANet)]


* [2019 Information Fusion] ğŸ”– [Xidian University, China] ğŸ”– **Affective video content analysis based on multimodal data fusion in heterogeneous networks**, Jie Guo, Bin Song,âˆ—, Peng Zhang, Mengdi Ma, Wenwen Luo, Junmei lv
  [[Paper](https://doi.org/10.1016/j.inffus.2019.02.007)] 

* [2019 ACM MM] ğŸ”– [University of Science and Technology of China] ğŸ”– **Multimodal Deep Denoise Framework for Affective Video Content Analysis**, Yaochen Zhu, Zhenzhong Chen, Feng Wu 
  [[Paper](https://dl.acm.org/doi/10.1145/3343031.3350997)] 

 

### ğŸŒ Open-world Video Emotion Spotting

* [2023 CVPR] ğŸ”– [ankai University] ğŸ”– **Weakly Supervised Video Emotion Detection and Prediction via Cross-Modal Temporal Erasing Network**, Zhicheng Zhang; Lijuan Wang; Jufeng Yang
  [[Paper](https://ieeexplore.ieee.org/document/10203999)] [[Code](https://github.com/nku-zhichengzhang/WECL)]

* [2022 ACM MM] ğŸ”– [Zhejiang University, China] ğŸ”– **Dilated Context Integrated Network with Cross-Modal Consensus for Temporal motion Localization in Videos**, juncheng Li, Junlin Xie, Linchao Zhu, Long Qian, Siliang Tang, Wenqiao Zhang, Haochen Shi, Shengyu Zhang, Longhui Wei, Qi Tian, Yueting Zhuang 
  [[Paper](https://dl.acm.org/doi/10.1145/3503161.3547886)]  [[Code](https://github.com/YYJMJC/Temporal-Emotion-Localization-in-Videos)]
  
### ğŸ˜Š Video-based Facial Emotion Recognition

* [2023 ACM MM] ğŸ”– [Politecnico di Torino, Italy] ğŸ”– **ViPER: Video-based Perceiver for Emotion Recognition**, Lorenzo Vaiani, Moreno La Quatra, Luca Cagliero, Paolo Garza
  [[Paper](https://dl.acm.org/doi/10.1145/3551876.3554806)]

* [2023 ACM MM] ğŸ”– [Fudan University, China] ğŸ”– **Freq-HD: An Interpretable Frequency-based High-Dynamics Affective Clip Selection Method for in-the-Wild Facial Expression Recognition in Videos**,  Zeng Tao, Yan Wang, Zhaoyu Chen, Boyang Wang, Shaoqi Yan, Kaixun Jiang, Shuyong Gao, Wenqiang Zhang
  [[Paper](https://dl.acm.org/doi/10.1145/3581783.3611972)] 
  
* [2020 AAAI] ğŸ”– [The Hong Kong University of Science and Technology, Kowloon, HK] ğŸ”– **MIMAMO Net: Integrating Micro- and Macro-Motion for Video Emotion Recognition**, Didan Deng, Zhaokang Chen, Yuqian Zhou, Bertram Shi
  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/5646)] [[Code](https://github.com/wtomin/MIMAMO-Net)]



### ğŸ˜Š Video-based Facial Emotion Editing  
* [2023 CVPR] ğŸ”– [National Technical University of Athens, Greece] ğŸ”– **Neural Emotion Director: Speech-preserving semantic control of facial expressions in "in-the-wild"videos**, Foivos Paraperas Papantoniou; Panagiotis P. Filntisis; Petros Maragos; Anastasios Roussos
  [[Paper](https://ieeexplore.ieee.org/document/9880027)] [[Code](https://foivospar.github.io/NED/)]

* [2021 ACM MM] ğŸ”– [Media Lab (MIT), USA] ğŸ”– **Invertable Frowns: Video-to-Video Facial Emotion Translation**, Ian Magnusson, Aruna Sankaranarayanan, Andrew Lippman
  [[Paper](https://dl.acm.org/doi/10.1145/3476099.3484317)]  [[Code](https://github.com/jagnusson/Wav2Lip-Emotion)]
  
### ğŸ˜Š Emotion-Video-based Talking Face Generation

* [2023 AAAI] ğŸ”– [THUAI] ğŸ”– **StyleTalk: One-Shot Talking Head Generation with Controllable Speaking Styles**, Yifeng Ma1, Suzhen Wang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Zhidong Deng, Xin Yu
  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/25280)]  [[Code](https://github.com/FuxiVirtualHuman/styletalk)]

* [2023 CVPR] ğŸ”– [BNRist and school of software, Tsinghua University] ğŸ”– **Audio-Driven Emotional Video Portraits**, Xinya Ji; Hang Zhou; Kaisiyuan Wang; Wayne Wu; Chen Change Loy; Xun Cao; Feng Xu
  [[Paper](https://ieeexplore.ieee.org/document/9578513)] [[Code](https://jixinya.github.io/projects/evp/)]

* [2023 CVPR] ğŸ”– [Microsoft/HKUST] ğŸ”–**MetaPortrait: ldentity-Preserving Talking Head Generation with Fast Personalized Adaptation**, Bowen Zhang, Chenyang Qi, Pan Zhang1 Bo Zhang, HsiangTao Wu, Dong Chen, Qifeng Chen, Yong Wang, Fang Wen 
  [[Paper](https://ieeexplore.ieee.org/document/10205008)]  [[Code](https://meta-portrait.github.io/)]


* [2023 CVPR] ğŸ”– [Xiaobing.AI] ğŸ”– **Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis**, Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, Baoyuan Wang 
  [[Paper](https://arxiv.org/pdf/2211.14506.pdf)]  [[Code](https://dorniwang.github.io/PD-FGC/)]

  
* [2023 CVPR] ğŸ”– [Shanghai AI Laboratory/Northwestern Polytechnical University] ğŸ”– **One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field**, Weichuang Li; Longhao Zhang; Dong Wang; Bin Zhao; Zhigang Wang; Mulin Chen; Bang Zhang; Zhongjian Wang; Liefeng Bo; Xuelong Li 
  [[Paper](https://ieeexplore.ieee.org/document/10203662)]  [[Code](https://www.waytron.net/hidenerf/)]

  
* [2023 CVPR] ğŸ”– [Microsoft Research] ğŸ”– **High-Fidelity and Freely Controllable Talking Head video Generation**, Yue Gao, Yuan Zhou, Jinglu Wang, Xiao Li, Xiang Ming, Yan Lu 
  [[Paper](https://ieeexplore.ieee.org/document/10204552)] 



* [2022 CVPR] ğŸ”– [Department of Computer Vision Technology (VIS), Baidu Inc.] ğŸ”– **Expressive Talking Head Generation with Granular Audio-Visual Control**, Borong Liang; Yan Pan; Zhizhi Guo; Hang Zhou; Zhibin Hong; Xiaoguang Han; Junyu Han; Jingtuo Liu; Errui Ding; Jingdong Wang 
  [[Paper](https://ieeexplore.ieee.org/document/9878472)] 

  
* [2022 CVPR] ğŸ”– [HKUST] ğŸ”– **Depth-Aware Generative Adversarial Network for Talking Head Video Generation**, Fa-Ting Hong; Longhao Zhang; Li Shen; Dan Xu 
  [[Paper](https://ieeexplore.ieee.org/document/9879781)]  [[Code](https://github.com/harlanhong/CVPR2022-DaGAN)]


* [2021 AAAI] ğŸ”– [Netease Fuxi AI Lab] ğŸ”– **Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation**, Lincheng Li, Suzhen Wang, Zhimeng Zhang, Yu Ding, Yixing Zheng, Xin Yu, Changjie Fan
  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/16286)]  [[Code](https://github.com/FuxiVirtualHuman/Write-a-Speaker)]


* [2021 CVPR] ğŸ”– [NVIDIA Corporation] ğŸ”– **One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing**, Ting-Chun Wang, Arun Mallya, Ming-Yu Liu 
  [[Paper](https://ieeexplore.ieee.org/document/9578110)]  [[Code](https://nvlabs.github.io/face-vid2vid)]

  
* [2020 CVPR] ğŸ”– [McMaster University] ğŸ”– **DAVD-Net: Deep Audio-Aided Video Decompression of Talking Heads**, Xi Zhang; Xiaolin Wu; Xinliang Zhai; Xianye Ben; Chengjie Tu 
  [[Paper](https://ieeexplore.ieee.org/document/9156859)] 

.....















